# List of variables to match
by.x  = "USAFID",
by.y  = "USAF",
# Which obs to keep?
all.x = TRUE,
all.y = FALSE
) %>% nrow()
dat <- merge(
# Data
x     = met,
y     = stations,
# List of variables to match
by.x  = "USAFID",
by.y  = "USAF",
# Which obs to keep?
all.x = TRUE,
all.y = FALSE
)
View(met)
View(dat)
# Download the data
stations <- fread("ftp://ftp.ncdc.noaa.gov/pub/data/noaa/isd-history.csv")
stations[, USAF := as.integer(USAF)]
# Dealing with NAs and 999999
stations[, USAF   := fifelse(USAF == 999999, NA_integer_, USAF)]
stations[, CTRY   := fifelse(CTRY == "", NA_character_, CTRY)]
stations[, STATE  := fifelse(STATE == "", NA_character_, STATE)]
# Selecting the three relevant columns, and keeping unique records
stations <- unique(stations[, list(USAF, CTRY, STATE)])
# Dropping NAs
stations <- stations[!is.na(USAF)]
# Removing duplicates
stations[, n := 1:.N, by = .(USAF)]
stations <- stations[n == 1,][, n := NULL]
# Merge Data
met <-merge(
# Data
x     = met,
y     = stations,
# List of variables to match
by.x  = "USAFID",
by.y  = "USAF",
# Which obs to keep?
all.x = TRUE,
all.y = FALSE
)
View(stations)
# create a new table contains three different spatial levels
pm_spatial= pm%>%
select(STATE,COUNTY,`Site Name`)
rename(site_name = "Site Name")
station_averages <-
met [, .(
temp = mean(temp, na.rm = T),
wind.sp =  mean(wind.sp, na.rm = T),
atm.press = mean(atm.press, na.rm = T),
)]
station_averages <-
met [ , .(
temp = mean(temp, na.rm = T),
wind.sp =  mean(wind.sp, na.rm = T),
atm.press = mean(atm.press, na.rm = T),
)]
station_averages <-
met[, .(
temp = mean(temp, na.rm=T),
wind.sp = mean(wind.sp, na.rm=T),
atm.press = mean(atm.press, na.rm=T)
), by = "USAFID"]
View(station_averages)
length(unique(met[,USAFID]))
nrow(met)
if (!file.exists("met_all.gz"))
download.file(
url = "https://raw.githubusercontent.com/USCbiostats/data-science-data/master/02_met/met_all.gz",
destfile = "met_all.gz",
method   = "libcurl",
timeout  = 60
)
met <- data.table::fread("met_all.gz")
met <- met[temp >= -17][elev == 9999.0, elev := NA]
# Download the data
stations <- fread("ftp://ftp.ncdc.noaa.gov/pub/data/noaa/isd-history.csv")
stations[, USAF := as.integer(USAF)]
# Dealing with NAs and 999999
stations[, USAF   := fifelse(USAF == 999999, NA_integer_, USAF)]
stations[, CTRY   := fifelse(CTRY == "", NA_character_, CTRY)]
stations[, STATE  := fifelse(STATE == "", NA_character_, STATE)]
# Selecting the three relevant columns, and keeping unique records
stations <- unique(stations[, list(USAF, CTRY, STATE)])
# Dropping NAs
stations <- stations[!is.na(USAF)]
# Removing duplicates
stations[, n := 1:.N, by = .(USAF)]
stations <- stations[n == 1,][, n := NULL]
# Merge Data
met <-merge(
# Data
x     = met,
y     = stations,
# List of variables to match
by.x  = "USAFID",
by.y  = "USAF",
# Which obs to keep?
all.x = TRUE,
all.y = FALSE
)
station_averages <-
met[, .(
temp = mean(temp, na.rm=T),
wind.sp = mean(wind.sp, na.rm=T),
atm.press = mean(atm.press, na.rm=T)
), by = "USAFID"]
length(unique(met[,USAFID]))
station_averages[, .(
temp50 = median(temp,na.rm=T),
windsp50 = median(wind.sp, rm=T),
atmpress50 = median(atm.press, rm=T)
)]
View(station_averages)
station_averages <-
met[, .(
temp = mean(temp, na.rm=T),
wind.sp = mean(wind.sp, na.rm=T),
atm.press = mean(atm.press, na.rm=T)
), by = "USAFID"]
statemeds <- station_averages[, .(
temp50 = median(temp,na.rm=T),
windsp50 = median(wind.sp, rm=T),
atmpress50 = median(atm.press, rm=T)
)]
View(statemeds)
View(statemeds)
View(station_averages)
# Download the data
stations <- fread("ftp://ftp.ncdc.noaa.gov/pub/data/noaa/isd-history.csv")
stations[, USAF := as.integer(USAF)]
# Dealing with NAs and 999999
stations[, USAF   := fifelse(USAF == 999999, NA_integer_, USAF)]
stations[, CTRY   := fifelse(CTRY == "", NA_character_, CTRY)]
stations[, STATE  := fifelse(STATE == "", NA_character_, STATE)]
# Selecting the three relevant columns, and keeping unique records
stations <- unique(stations[, list(USAF, CTRY, STATE)])
# Dropping NAs
stations <- stations[!is.na(USAF)]
# Removing duplicates
stations[, n := 1:.N, by = .(USAF)]
stations <- stations[n == 1,][, n := NULL]
# Merge Data
met <-merge(
# Data
x     = met,
y     = stations,
# List of variables to match
by.x  = "USAFID",
by.y  = "USAF",
# Which obs to keep?
all.x = TRUE,
all.y = FALSE
)
station_averages <-
met[, .(
temp = mean(temp, na.rm=T),
wind.sp = mean(wind.sp, na.rm=T),
atm.press = mean(atm.press, na.rm=T)
), by = "USAFID"]
statemeds <- station_averages[, .(
temp50 = median(temp,na.rm=T),
windsp50 = median(wind.sp, rm=T),
atmpress50 = median(atm.press, rm=T)
)]
View(statemeds)
station_averages[ ,
temp_dist50 := abs(temp - statmedian$temp50)]
station_averages[ ,
temp_dist50 := abs(temp - statmeds$temp50)]
station_averages[ ,
temp_dist50 := abs(temp - statemeds$temp50)]
station_averages[ ,
temp_dist50 := abs(temp - statemeds$temp50)][order(temp_dist50)]
statemeds <- station_averages[, .(
temp50 = median(temp,na.rm=T),
windsp50 = median(wind.sp, na.rm=T),
atmpress50 = median(atm.press, na.rm=T)
)]
station_averages[ ,
temp_dist50 := abs(temp - statemeds$temp50)][order(temp_dist50)]
View(statemeds)
# create a new table contains three different spatial levels
pm_spatial= pm%>%
select(STATE,COUNTY,`Site Name`)
# combine state and county together as a new column
pm_spatial <- mutate(pm_spatial, location = paste(STATE, COUNTY, sep = "."))
# create the boxplot
library(ggplot2)
boxplot(location, "Site name")
station_averages[which.min(temp_dist50)]
station_averages[ ,
windsp_dist50 := abs(wind.sp - statemeds$windsp50)][order(windsp_dist50)]
station_averages[which.min(windsp_dist50)]
station_averages[ ,
temp_dist50 := abs(temp - statemeds$temp50)][order(temp_dist50)]
station_averages[which.min(temp_dist50)]
station_averages[ ,
windsp_dist50 := abs(wind.sp - statemeds$windsp50)][order(windsp_dist50)]
station_averages[which.min(windsp_dist50)]
station_averages[ ,
atmpress_dist50 := abs(atm.press - statemeds$atmpress50)][order(atmpress_dist50)]
station_averages[which.min(atmpress_dist50)]
View(pm_spatial)
View(pm)
# create a new table contains three different spatial levels
pm_spatial= pm%>%
select(STATE,COUNTY,`Site Name`, pm2.5)
# combine state and county together as a new column
pm_spatial <- mutate(pm_spatial, location = paste(STATE, COUNTY, sep = "."))
# create the boxplot
library(ggplot2)
set.seed(2015)
idx <- sample(nrow(pm_spatial), 1000)
# create a new table contains three different spatial levels
pm_spatial= pm%>%
select(STATE,COUNTY,`Site Name`, pm2.5)
# combine state and county together as a new column
pm_spatial <- mutate(pm_spatial, location = paste(STATE, COUNTY, sep = "."))
# create the boxplot
library(ggplot2)
qplot(location, pm2.5, geom = "boxplot")
# create a new table contains three different spatial levels
pm_spatial= pm%>%
select(STATE,COUNTY,`Site Name`, pm2.5)
# combine state and county together as a new column
pm_spatial <- mutate(pm_spatial, location = paste(STATE, COUNTY, sep = "."))
# create the boxplot
library(ggplot2)
boxplot(location, pm2.5)
View(pm_spatial)
# create a new table contains three different spatial levels
pm_spatial= pm%>%
select(STATE,COUNTY,`Site Name`, pm2.5)
# combine state and county together as a new column
pm <- mutate(pm, location = paste(STATE, COUNTY, sep = "."))
# create the boxplot
boxplot(location, pm2.5)
station_averages <-
met[, .(
temp = mean(temp, na.rm=T),
wind.sp = mean(wind.sp, na.rm=T),
atm.press = mean(atm.press, na.rm=T)
), by = .(USAFID,STATE)]
View(met)
station_averages <-
met[, .(
temp = mean(temp, na.rm=T),
wind.sp = mean(wind.sp, na.rm=T),
atm.press = mean(atm.press, na.rm=T)
), by = .(USAFID,STATE.y)]
statemeds <- station_averages[, .(
temp50 = median(temp,na.rm=T),
windsp50 = median(wind.sp, na.rm=T),
atmpress50 = median(atm.press, na.rm=T)
), by = STATE.y]
dim(statemeds)
statemeds <- station_averages[, .(
temp50 = median(temp,na.rm=T),
windsp50 = median(wind.sp, na.rm=T),
atmpress50 = median(atm.press, na.rm=T)
), by = STATE.y]
statemeds
statedist <- station_averages[,
temp_diststate50 = temp - statemeds$temp50,
by STATE.y]
statemeds <- station_averages[, .(
temp50 = median(temp,na.rm=T),
windsp50 = median(wind.sp, na.rm=T),
atmpress50 = median(atm.press, na.rm=T)
), by = STATE.y][order(STATE.y)]
statemeds
station_averages[ ,
temp_dist50 := abs(temp - statemeds$temp50)][order(temp_dist50)]
station_averages[which.min(temp_dist50)]
statedist <- station_averages[,
temp_diststate50 = temp - statemeds$temp50,
by STATE.y]
statemeds <- station_averages[, .(
temp50 = median(temp,na.rm=T),
windsp50 = median(wind.sp, na.rm=T),
atmpress50 = median(atm.press, na.rm=T)
), by = STATE.y]
statemeds
library(data.table)
# read 2004 data
data_2004 <- fread("data/data_2004.csv")
pm_2004 <- data.table(data_2004, keep.rownames=FALSE, check.names=FALSE, key=NULL, stringsAsFactors=FALSE)
# read 2019 data
data_2019 <- fread("data/data_2019.csv")
pm_2019 <- data.table(data_2019, keep.rownames=FALSE, check.names=FALSE, key=NULL, stringsAsFactors=FALSE)
# check 2004 data
# check the dimension
dim(pm_2004)
# check the header
head(pm_2004)
# check the footer
tail(pm_2004)
# check the variable name
colnames(pm_2004)
# check the variable type
str(pm_2004)
# check the summary
summary(pm_2004)
library(dplyr)
# Combine the two years of data into one data frame
pm <- rbind(pm_2004, pm_2019)
# create a new column for year, which will serve as an identifier
pm <- mutate(pm, year = factor(rep(c(2004, 2019), c(nrow(pm_2004), nrow(pm_2019)))))%>%
rename(lat = SITE_LATITUDE, lon = SITE_LONGITUDE, pm2.5 = "Daily Mean PM2.5 Concentration")
statemeds <- station_averages[, .(
temp50 = median(temp, na.rm=T),
wind.sp50 = median(wind.sp, na.rm=T),
), by = STATE]
statemeds <- station_averages[, .(
temp50 = median(temp, na.rm=T),
wind.sp50 = median(wind.sp, na.rm=T),
), by = STATE.y]
statemeds <- station_averages[, .(
temp50 = median(temp,na.rm=T),
windsp50 = median(wind.sp, na.rm=T),
atmpress50 = median(atm.press, na.rm=T)
), by = STATE.y]
statemeds
station_averages[, temp_dist50 := temp - statemeds$temp50, by = STATE.y]
station_averages[, temp_dist50 := temp - statemeds$temp50]
station_averages[, windsp_dist50 := wind.sp - statemeds$windsp50]
statemeds <- station_averages[, .(
temp50 = median(temp,na.rm=T),
windsp50 = median(wind.sp, na.rm=T),
), by = STATE.y]
station_averages <-
met[, .(
temp = mean(temp, na.rm=T),
wind.sp = mean(wind.sp, na.rm=T),
atm.press = mean(atm.press, na.rm=T)
), by = .(USAFID,STATE.y)]
statemeds <- station_averages[, .(
temp50 = median(temp,na.rm=T),
windsp50 = median(wind.sp, na.rm=T),
), by = STATE.y]
statemeds <- station_averages[, .(
temp50 = median(temp,na.rm=T),
windsp50 = median(wind.sp, na.rm=T)
), by = STATE.y]
statemeds
station_averages[, temp_dist50 := temp - statemeds$temp50]
station_averages[, windsp_dist50 := wind.sp - statemeds$windsp50]
dim(station_averages)
station_averages <-
met[, .(
temp = mean(temp, na.rm=T),
wind.sp = mean(wind.sp, na.rm=T),
atm.press = mean(atm.press, na.rm=T)
), by = .(USAFID,STATE.y)]
dim(station_averages)
station_averages[, temp_dist50 := temp - statemeds$temp50]
station_averages[, windsp_dist50 := wind.sp - statemeds$windsp50]
dim(statemeds)
station_averages <-
met[, .(
temp = mean(temp, na.rm=T),
wind.sp = mean(wind.sp, na.rm=T),
), by = .(USAFID,STATE.y)]
station_averages <-
met[, .(
temp = mean(temp, na.rm=T),
wind.sp = mean(wind.sp, na.rm=T)
), by = .(USAFID,STATE.y)]
statemeds <- station_averages[, .(
temp50 = median(temp,na.rm=T),
windsp50 = median(wind.sp, na.rm=T)
), by = STATE.y]
statemeds
station_averages[, temp_dist50 := temp - statemeds$temp50]
station_averages[, windsp_dist50 := wind.sp - statemeds$windsp50]
station_averages[, temp_dist_state50  := temp - temp_dist50]
station_averages[, windsp_dist_state50 := wind.sp - windsp50]
statemeds <- station_averages[, .(
temp50 = median(temp,na.rm=T),
windsp50 = median(wind.sp, na.rm=T)
), by = STATE.y]
statemeds
station_averages[, temp_dist_state50  := temp - temp_dist50]
station_averages[, windsp_dist_state50 := wind.sp - windsp50]
# Merge Data
met <-merge(
# Data
x     = station_averages,
y     = statemeds,
# List of variables to match
by.x  = "USAFID",
by.y  = "USAF",
# Which obs to keep?
all.x = TRUE,
all.y = FALSE
)
View(station_averages)
# Merge Data
met <-merge(
# Data
x     = station_averages,
y     = statemeds,
# List of variables to match
by.x  = "USAFID",
by.y  = "STATE.y",
# Which obs to keep?
all.x = TRUE,
all.y = FALSE
)
# Download the data
stations <- fread("ftp://ftp.ncdc.noaa.gov/pub/data/noaa/isd-history.csv")
stations[, USAF := as.integer(USAF)]
# Dealing with NAs and 999999
stations[, USAF   := fifelse(USAF == 999999, NA_integer_, USAF)]
stations[, CTRY   := fifelse(CTRY == "", NA_character_, CTRY)]
stations[, STATE  := fifelse(STATE == "", NA_character_, STATE)]
# Selecting the three relevant columns, and keeping unique records
stations <- unique(stations[, list(USAF, CTRY, STATE)])
# Dropping NAs
stations <- stations[!is.na(USAF)]
# Removing duplicates
stations[, n := 1:.N, by = .(USAF)]
stations <- stations[n == 1,][, n := NULL]
# Merge Data
met <-merge(
# Data
x     = met,
y     = stations,
# List of variables to match
by.x  = "USAFID",
by.y  = "USAF",
# Which obs to keep?
all.x = TRUE,
all.y = FALSE
)
gc()
library(data.table)
# read 2004 data
data_2004 <- fread("data/data_2004.csv")
pm_2004 <- data.table(data_2004, keep.rownames=FALSE, check.names=FALSE, key=NULL, stringsAsFactors=FALSE)
# read 2019 data
data_2019 <- fread("data/data_2019.csv")
pm_2019 <- data.table(data_2019, keep.rownames=FALSE, check.names=FALSE, key=NULL, stringsAsFactors=FALSE)
# check 2004 data
# check the dimension
dim(pm_2004)
# check the header
head(pm_2004)
# check the footer
tail(pm_2004)
# check the variable name
colnames(pm_2004)
# check the variable type
str(pm_2004)
# check the summary
summary(pm_2004)
library(dplyr)
# Combine the two years of data into one data frame
pm <- rbind(pm_2004, pm_2019)
# create a new column for year, which will serve as an identifier
pm <- mutate(pm, year = factor(rep(c(2004, 2019), c(nrow(pm_2004), nrow(pm_2019)))))%>%
rename(lat = SITE_LATITUDE, lon = SITE_LONGITUDE, pm2.5 = "Daily Mean PM2.5 Concentration")
library(leaflet)
#filter locations and year of the table to create a new table
pm_new <- pm
pm_new = pm_new%>%
select(lat,lon,year)
# Generating a color palette
pm.pal <- colorFactor(c('blue','red'), domain=pm_new$year)
# Create a basic map in leaflet() that shows the locations of the sites
pmmap <- leaflet(pm_new) %>%
# The looks of the Map
addProviderTiles('CartoDB.Positron') %>%
# Some circles
addCircles(
lat = ~lat, lng=~lon,
label = ~paste0(year, ' C'), color = ~ pm.pal(year),
opacity = 1, fillOpacity = 1, radius = 2
) %>%
# And a pretty legend
addLegend('bottomleft', pal=pm.pal, values=pm_new$year,
title='Year')
pmmap
# Check for any missing values of PM in the combined dataset
which(is.na(pm))
# Check for any implausible values of PM in the combined dataset
which(is.nan(pm))
View(pm)
# Check for any missing values of PM in the combined dataset
which(is.na(pm$Date) )
# Check for any missing values of PM in the combined dataset
which(is.na(pm$COUNTY) )
# Check for any missing values of PM in the combined dataset
which(is.na(pm$`Site Name`) )
# Check for any missing values of PM in the combined dataset
apply(is.na(pm), 2, which)
# Check for any missing values of PM in the combined dataset
apply(is.na(pm), 2, which)
apply(is.nan(pm), 2, which)
# Check for any missing values of PM in the combined dataset
apply(is.na(pm), 2, which)
is.nan(as.matrix(pm))
# Check for any missing values of PM in the combined dataset
apply(is.na(pm), 2, which)
apply(is.nan(as.matrix(pm)), 2, which)
