plot(1:100, freqs1, xlab="generation number", ylab="allele frequency", type="l", col="red")
lines(1:100, freqs2, col="green")
lines(1:100, freqs3, col="blue")
# population size: 10 individuals, 50% initial allele frequency, 100 generations
freqs1 <- allele.frequency(10, 0.5, 100)
# population size: 100 individuals, 50% initial allele frequency, 100 generations
freqs2 <- allele.frequency(100, 0.5, 100)
# population size: 1000 individuals, 50% initial allele frequency, 100 generations
freqs3 <- allele.frequency(1000, 0.5, 100)
# plot the first graph
plot(1:100, freqs1, xlab="generation number", ylab="allele frequency", type="l", col="red")
lines(1:100, freqs2, col="green")
lines(1:100, freqs3, col="blue")
# population size: 10 individuals, 50% initial allele frequency, 100 generations
freqs1 <- allele.frequency(10, 0.5, 100)
# population size: 100 individuals, 50% initial allele frequency, 100 generations
freqs2 <- allele.frequency(100, 0.5, 100)
# population size: 1000 individuals, 50% initial allele frequency, 100 generations
freqs3 <- allele.frequency(1000, 0.5, 100)
# plot the first graph
plot(1:100, freqs1, xlab="generation number", ylab="allele frequency", type="l", col="red")
lines(1:100, freqs2, col="green")
lines(1:100, freqs3, col="blue")
gc()
library(data.table)
# read 2004 data
data_2004 <- fread("data/data_2004.csv")
pm_2004 <- data.table(data_2004, keep.rownames=FALSE, check.names=FALSE, key=NULL, stringsAsFactors=FALSE)
# read 2019 data
data_2019 <- fread("data/data_2019.csv")
pm_2019 <- data.table(data_2019, keep.rownames=FALSE, check.names=FALSE, key=NULL, stringsAsFactors=FALSE)
# check 2004 data
# check the dimension
dim(pm_2004)
# check the header
head(pm_2004)
# check the footer
tail(pm_2004)
# check the variable name
colnames(pm_2004)
# check the variable type
str(pm_2004)
# check the summary
summary(pm_2004)
# check 2019 data
# check the dimension
dim(pm_2019)
# check the header
head(pm_2019)
# check the footer
tail(pm_2019)
# check the variable name
colnames(pm_2019)
# check the variable type
str(pm_2019)
# check the summary
summary(pm_2019)
library(dplyr)
# Combine the two years of data into one data frame
pm <- rbind(pm_2004, pm_2019)
# create a new column for year, which will serve as an identifier
pm <- mutate(pm, year = factor(rep(c(2004, 2019), c(nrow(pm_2004), nrow(pm_2019)))))%>%
rename(lat = SITE_LATITUDE, lon = SITE_LONGITUDE, pm2.5 = "Daily Mean PM2.5 Concentration")
library(leaflet)
#filter locations and year of the table to create a new table
pm_new <- pm
pm_new = pm_new%>%
select(lat,lon,year)
# Generating a color palette
pm.pal <- colorFactor(c('blue','red'), domain=pm_new$year)
# Create a basic map in leaflet() that shows the locations of the sites
pmmap <- leaflet(pm_new) %>%
# The looks of the Map
addProviderTiles('CartoDB.Positron') %>%
# Some circles
addCircles(
lat = ~lat, lng=~lon,
label = ~paste0(year, ' C'), color = ~ pm.pal(year),
opacity = 1, fillOpacity = 1, radius = 2
) %>%
# And a pretty legend
addLegend('bottomleft', pal=pm.pal, values=pm_new$year,
title='Year')
pmmap
# Check for any missing values of PM in the combined dataset
apply(is.na(pm), 2, which)
# Check for any implausible values of PM in the combined dataset
apply(is.nan(as.matrix(pm)), 2, which)
boxplot(pm$year, pm$pm2.5)
library(ggplot2)
set.seed(2015)
idx <- sample(nrow(pm), 1000)
qplot(year, log2(PM), data = pm[idx, ], geom = "boxplot")
library(ggplot2)
set.seed(2015)
idx <- sample(nrow(pm), 1000)
qplot(year, pm2.5, data = pm[idx, ], geom = "boxplot")
library(ggplot2)
# analysis trend based on state level
set.seed(2015)
idx <- sample(nrow(pm), 1000)
qplot(year, pm2.5, data = pm[idx, ], geom = "boxplot")
# do statistical analysis
with(pm, tapply(pm2.5, year, summary))
library(ggplot2)
# analysis trend based on state level
set.seed(2015)
idx <- sample(nrow(pm), 1000)
qplot(year, pm2.5, data = pm[idx, ], geom = "boxplot")
# do statistical analysis
with(pm, tapply(pm2.5, year, summary))
# do statistical analysis
with(pm, tapply(pm2.5, year, summary))
filter(pm, year == "2019") %>% summarize(negative = mean(PM2.5 < 0, na.rm = TRUE))
filter(pm, year == "2019") %>% summarize(negative = mean(pm2.5 < 0, na.rm = TRUE))
filter(pm, year == "2019") %>% summarize(negative = mean(pm2.5 < 0, na.rm = TRUE))
library(lubridate)
negative <- filter(pm, year == "2019") %>%
mutate(negative = pm2.5 < 0, date = ymd(Date)) %>%
select(date, negative)
filter(pm, year == "2019") %>% summarize(negative = mean(pm2.5 < 0, na.rm = TRUE))
library(lubridate)
negative <- filter(pm, year == "2019") %>%
mutate(negative = pm2.5 < 0, date = Date) %>%
select(date, negative)
filter(pm, year == "2019") %>% summarize(negative = mean(pm2.5 < 0, na.rm = TRUE))
library(lubridate)
negative <- filter(pm, year == "2019") %>%
mutate(negative = pm2.5 < 0, date = Date) %>%
select(date, negative)
mutate(negative, month = factor(month.name[month(date)], levels = month.name)) %>%
group_by(month) %>%
summarize(pct.negative = mean(negative, na.rm = TRUE) * 100)
filter(pm, year == "2019") %>% summarize(negative = mean(pm2.5 < 0, na.rm = TRUE))
library(lubridate)
negative <- filter(pm, year == "2019") %>%
mutate(negative = pm2.5 < 0, date = ymd(Date)) %>%
select(date, negative)
mutate(negative, month = factor(month.name[month(date)], levels = month.name)) %>%
group_by(month) %>%
summarize(pct.negative = mean(negative, na.rm = TRUE) * 100)
library(data.table)
# read 2004 data
data_2004 <- fread("data/data_2004.csv")
pm_2004 <- data.table(data_2004, keep.rownames=FALSE, check.names=FALSE, key=NULL, stringsAsFactors=FALSE)
cnames <- readLines("data/data_2004.csv", 1)
cnames <- strsplit(cnames, "|", fixed = TRUE)
## Ensure names are properly formatted
names(pm_2004) <- make.names(cnames[[1]])
library(data.table)
# read 2004 data
data_2004 <- fread("data/data_2004.csv")
pm_2004 <- data.table(data_2004, keep.rownames=FALSE, check.names=FALSE, key=NULL, stringsAsFactors=FALSE)
cnames <- readLines("data/data_2004.csv", 1)
cnames <- strsplit(cnames, "|", fixed = TRUE)
# read 2019 data
data_2019 <- fread("data/data_2019.csv")
pm_2019 <- data.table(data_2019, keep.rownames=FALSE, check.names=FALSE, key=NULL, stringsAsFactors=FALSE)
cnames <- readLines("data/data_2019.csv", 1)
cnames <- strsplit(cnames, "|", fixed = TRUE)
View(cnames)
filter(pm, year == "2019") %>% summarize(negative = mean(pm2.5 < 0, na.rm = TRUE))
library(lubridate)
negative <- filter(pm, year == "2019") %>%
mutate(negative = pm2.5 < 0, date = as.Date(Date)) %>%
select(date, negative)
mutate(negative, month = factor(month.name[month(date)], levels = month.name)) %>%
group_by(month) %>%
summarize(pct.negative = mean(negative, na.rm = TRUE) * 100)
missing_data <- filter(pm, CBSA_CODE == "") %>%
select("Site name", CBSA_CODE, STATE, COUNTY, year)
missing_data <- filter(pm, CBSA_CODE == "") %>%
select("Site Name", CBSA_CODE, STATE, COUNTY, year)
unique(missing_data)
View(missing_data)
missing_data <- filter(pm, CBSA_CODE == "") %>%
select("Site Name", CBSA_CODE, STATE, COUNTY, year)
missing_data <- filter(pm, CBSA_CODE == " ") %>%
select("Site Name", CBSA_CODE, STATE, COUNTY, year)
missing_data <- filter(pm, CBSA_CODE == "NA") %>%
select("Site Name", CBSA_CODE, STATE, COUNTY, year)
missing_data <- filter(pm, CBSA_CODE == NA) %>%
select("Site Name", CBSA_CODE, STATE, COUNTY, year)
library(data.table)
# read 2004 data
data_2004 <- fread("data/data_2004.csv", na = "")
pm_2004 <- data.table(data_2004, keep.rownames=FALSE, check.names=FALSE, key=NULL, stringsAsFactors=FALSE)
# read 2019 data
data_2019 <- fread("data/data_2019.csv", na = "")
pm_2019 <- data.table(data_2019, keep.rownames=FALSE, check.names=FALSE, key=NULL, stringsAsFactors=FALSE)
library(dplyr)
# Combine the two years of data into one data frame
pm <- rbind(pm_2004, pm_2019)
# create a new column for year, which will serve as an identifier
pm <- mutate(pm, year = factor(rep(c(2004, 2019), c(nrow(pm_2004), nrow(pm_2019)))))%>%
rename(lat = SITE_LATITUDE, lon = SITE_LONGITUDE, pm2.5 = "Daily Mean PM2.5 Concentration")
# Check for any missing values of PM in the combined dataset
apply(is.na(pm), 2, which)
# Check for any implausible values of PM in the combined dataset
apply(is.nan(as.matrix(pm)), 2, which)
missing_data <- filter(pm, CBSA_CODE == "") %>%
select("Site Name", CBSA_CODE, STATE, COUNTY, year)
library(data.table)
# read 2004 data
data_2004 <- fread("data/data_2004.csv")
pm_2004 <- data.table(data_2004, keep.rownames=FALSE, check.names=FALSE, key=NULL, stringsAsFactors=FALSE)
# read 2019 data
data_2019 <- fread("data/data_2019.csv")
pm_2019 <- data.table(data_2019, keep.rownames=FALSE, check.names=FALSE, key=NULL, stringsAsFactors=FALSE)
library(dplyr)
# Combine the two years of data into one data frame
pm <- rbind(pm_2004, pm_2019)
# create a new column for year, which will serve as an identifier
pm <- mutate(pm, year = factor(rep(c(2004, 2019), c(nrow(pm_2004), nrow(pm_2019)))))%>%
rename(lat = SITE_LATITUDE, lon = SITE_LONGITUDE, pm2.5 = "Daily Mean PM2.5 Concentration")
# Check for any missing values of PM in the combined dataset
apply(is.na(pm), 2, which)
# Check for any implausible values of PM in the combined dataset
apply(is.nan(as.matrix(pm)), 2, which)
missing_data <- filter(pm, CBSA_CODE == "NA") %>%
select("Site Name", CBSA_CODE, STATE, COUNTY, year)
missing_data <- filter(pm, CBSA_NAME == "") %>%
select("Site Name", CBSA_CODE, STATE, COUNTY, year)
missing_data <- filter(pm, CBSA_NAME == "") %>%
select("Site Name", CBSA_CODE, CBSA_NAME, STATE, COUNTY, year)
# create a new data frame that only contains missing data
missing_data <- filter(pm, CBSA_NAME == "") %>%
select("Site Name", CBSA_CODE, CBSA_NAME, STATE, COUNTY, year)
unique(missing_data)
# calculate the mean of PM2.5 for each county in 2004 and 2019
mn <- group_by(pm, year, COUNTY_Code) %>% summarize(pm2.5 = mean(pm2.5, na.rm = TRUE))
# calculate the mean of PM2.5 for each county in 2004 and 2019
mn <- group_by(pm, year, COUNTY_CODE) %>% summarize(pm2.5 = mean(pm2.5, na.rm = TRUE))
head(mn)
# calculate the mean of PM2.5 for each county in 2004 and 2019
mn <- group_by(pm, year, COUNTY_CODE) %>% summarize(pm2.5 = mean(pm2.5, na.rm = TRUE))
head(mn)
tail(mn)
# calculate the mean of PM2.5 for each county in 2004 and 2019
mn <- group_by(pm, year, COUNTY) %>% summarize(pm2.5 = mean(pm2.5, na.rm = TRUE))
head(mn)
tail(mn)
# make the plot to show mean and draw a line connecting the means for each year in the same county to highlight the trend
qplot(year, pm2.5, data = mutate(mn, year = as.numeric(as.character(year))),
color = factor(COUNTY),
geom = c("point", "line"))
# create a new data frame that only contains missing data
missing_data <- filter(pm, CBSA_NAME == "") %>%
select("Site Name", CBSA_CODE, CBSA_NAME, STATE, COUNTY, year)%>%
unique
# identify sites in Los Angeles that has data in 2004 and 2019
sites <- filter(pm, COUNTY.Code == 37) %>% select(Site.ID, "Site Name", year) %>% unique
# identify sites in Los Angeles that has data in 2004 and 2019
sites <- filter(pm, COUNTY.CODE == 37) %>% select(Site.ID, "Site Name", year) %>% unique
# identify sites in Los Angeles that has data in 2004 and 2019
sites <- filter(pm, COUNTY_CODE == 37) %>% select(Site.ID, "Site Name", year) %>% unique
# identify sites in Los Angeles that has data in 2004 and 2019
sites <- filter(pm, COUNTY_CODE == 37) %>% select("Site ID", "Site Name", year) %>% unique
View(sites)
# identify sites in Los Angeles that has data in 2004 and 2019
sites <- filter(pm, COUNTY_CODE == 37) %>% select("Site ID", "Site Name", year) %>% unique %>% rename(site_id = "Site ID", site_name = "Site Name")
# find the intersection between the sites present in 2004 and 2019
site.year <- with(sites, split(site_id, year))
both <- intersect(site.year[[1]], site.year[[2]])
print(both)
# choose one that had a reasonable amount of data in each year
count <- mutate(pm, site.id = "Site ID") %>%
filter(site.id %in% both)
View(count)
# choose one that had a reasonable amount of data in each year
count <- filter("Site ID" %in% both)
# choose one that had a reasonable amount of data in each year
count <- filter(pm, "Site ID" %in% both)
# identify sites in Los Angeles that has data in 2004 and 2019
sites <- filter(pm, COUNTY_CODE == 37) %>% select("Site ID", "Site Name", year) %>% unique %>% rename(site_id = "Site ID", site_name = "Site Name")
# find the intersection between the sites present in 2004 and 2019
site.year <- with(sites, split(site_id, year))
both <- intersect(site.year[[1]], site.year[[2]])
print(both)
# choose one that had a reasonable amount of data in each year
count <- filter(pm, "Site ID" %in% both)
# choose one that had a reasonable amount of data in each year
count <- filter(pm, "Site ID" == 60370002)
# choose one that had a reasonable amount of data in each year
count <- pm
# choose one that had a reasonable amount of data in each year
count <- pm
count <- filter(count, "Site ID" == 60370002)
# choose one that had a reasonable amount of data in each year
count <- pm
count <- filter(count, POC == 1)
# choose one that had a reasonable amount of data in each year
count <- pm
count <- filter(count, "Site ID" == 60010007)
# choose one that had a reasonable amount of data in each year
count <- pm
count <- filter(count, "Site ID" == "60010007")
# choose one that had a reasonable amount of data in each year
count <- pm %>%
rename(site_id = "Site ID")
View(count)
# check 2004 data
# check the dimension
dim(pm_2004)
# check the header
head(pm_2004)
# check the footer
tail(pm_2004)
# check the variable name
colnames(pm_2004)
# check the variable type
str(pm_2004)
# check the summary
summary(pm_2004)
# choose one that had a reasonable amount of data in each year
count <- pm %>%
rename(site_id = "Site ID") %>%
filter(count, site_id == 60370002)
# choose one that had a reasonable amount of data in each year
count <- pm %>%
rename(site_id = "Site ID") %>%
filter(site_id == 60370002)
# choose one that had a reasonable amount of data in each year
count <- pm %>%
rename(site_id = "Site ID") %>%
filter(site_id %in% both)
# choose one that had a reasonable amount of data in each year
count <- pm %>%
rename(site_id = "Site ID") %>%
filter(site_id %in% both)
# count the number of observations at each monitor to see which ones have the most observations
group_by(count, site.id) %>% summarize(n = n())
# choose one that had a reasonable amount of data in each year
count <- pm %>%
rename(site_id = "Site ID") %>%
filter(site_id %in% both)
# count the number of observations at each monitor to see which ones have the most observations
group_by(count, site_id) %>% summarize(n = n())
pmsub <- filter(pm, "Site ID" == "60371103") %>%
select(Date, year, pm2.5) %>%
mutate(Date = ymd(Date), yday = yday(Date))
View(pmsub)
pmsub <- filter(pm, "Site ID" == 60371103) %>%
select(Date, year, pm2.5) %>%
mutate(Date = ymd(Date), yday = yday(Date))
pmsub <- pm %>%
rename(site_id = "Site ID") %>%
filter(site_id== 60371103) %>%
select(Date, year, pm2.5) %>%
mutate(Date = ymd(Date), yday = yday(Date))
pmsub <- pm %>%
rename(site_id = "Site ID") %>%
filter(site_id== 60371103) %>%
select(Date, year, pm2.5) %>%
mutate(Date = as.Date(Date), yday = yday(Date))
# select monitor with site_id = 60371103
pmsub <- pm %>%
rename(site_id = "Site ID") %>%
filter(site_id== 60371103) %>%
select(Date, year, pm2.5) %>%
mutate(Date = as.Date(Date), yday = yday(Date))
# plot the time series data of PM2.5 for the monitor in both years
qplot(yday, pm2.5, data = pmsub, facets = . ~ year, xlab = "Day of the year")
# select monitor with site_id = 60371103
pmsub <- pm[!is.na(Date) & !is.na(year) & !is.na(pm2.5)] %>%
rename(site_id = "Site ID") %>%
filter(site_id== 60371103) %>%
select(Date, year, pm2.5) %>%
mutate(Date = as.Date(Date), yday = yday(Date))
# plot the time series data of PM2.5 for the monitor in both years
qplot(yday, pm2.5, data = pmsub, facets = . ~ year, xlab = "Day of the year")
# select monitor with site_id = 60371103
pmsub <- pm %>%
rename(site_id = "Site ID") %>%
filter(site_id== 60371103) %>%
select(Date, year, pm2.5) %>%
mutate(Date = as.Date(Date), yday = yday(Date))
# plot the time series data of PM2.5 for the monitor in both years
pmsub[!is.na(Date) & !is.na(year) & !is.na(pm2.5)] %>%
qplot(yday, pm2.5, data = pmsub, facets = . ~ year, xlab = "Day of the year")
# select monitor with site_id = 60371103
pmsub <- pm %>%
rename(site_id = "Site ID") %>%
filter(site_id== 60371103) %>%
select(Date, year, pm2.5) %>%
mutate(Date = as.Date(Date), yday = yday(Date))
# plot the time series data of PM2.5 for the monitor in both years
pmsub[!is.na(Date) & !is.na(year)] %>%
qplot(yday, pm2.5, data = pmsub, facets = . ~ year, xlab = "Day of the year")
# select monitor with site_id = 60371103
pmsub <- pm %>%
rename(site_id = "Site ID") %>%
filter(site_id== 60371103) %>%
select(Date, year, pm2.5) %>%
mutate(Date = as.Date(Date), yday = yday(Date))
# plot the time series data of PM2.5 for the monitor in both years
qplot(yday, pm2.5, data = pmsub, facets = . ~ year, xlab = "Day of the year")
knitr::opts_chunk$set(eval = FALSE, include  = FALSE)
install.packages('tidytext')
library(dplyr)
library(ggplot2)
library(tidytext)
mtsample <- read.csv("https://raw.githubusercontent.com/USCbiostats/data-science-data/master/00_mtsamples/mtsamples.csv")
mtsample <- read.csv("https://raw.githubusercontent.com/USCbiostats/data-science-data/master/00_mtsamples/mtsamples.csv")
View(mtsample)
count(mtsample)
mtsample %>%
unnest_tokens(token,text) %>%
count(token)
mtsample %>%
unnest_tokens(token,text)
ms <- mtsample %>%
count(medical_specialty)
View(ms)
# read dataset
mtsample <- read.csv("https://raw.githubusercontent.com/USCbiostats/data-science-data/master/00_mtsamples/mtsamples.csv")
mts <- as_tibble(mts)
# read dataset
mtsample <- read.csv("https://raw.githubusercontent.com/USCbiostats/data-science-data/master/00_mtsamples/mtsamples.csv")
mts <- as_tibble(mtsample)
ms <- mtsample %>%
count(medical_specialty)
ms %>%
arrange(desc(n)) %>%
knitr::kable()
ms %>%
top_n(10) %>%
ggplot(aes(x = n, y = fct_reorder(medical_specialty, n ))) +
geom_col()
library(dplyr)
library(ggplot2)
library(tidytext)
library(forcats)
ms %>%
top_n(10) %>%
ggplot(aes(x = n, y = fct_reorder(medical_specialty, n ))) +
geom_col()
mtsample %>%
unnest_tokens(word, transcription) %>%
count(word, sort = TRUE) %>%
top_n(20, n) %>%
ggplot(aes(n, fct_reorder(word, n))) +
geom_col()
mtsample %>%
unnest_tokens(word, transcription) %>%
anti_join(stop_words, by = c("word")) %>%
count(word, sort = TRUE) %>%
#use regular expression
filter(!grepl(pattern = "^[0-9]+$", x = word)) %>%
top_n(20, n) %>%
ggplot(aes(n, fct_reorder(word, n))) +
geom_col()
mtsample %>%
unnest_ngrams(bigram, transcription, n=2) %>%
count(bigram, sort = TRUE) %>%
top_n(20, n) %>%
ggplot(aes(n, fct_reorder(bigram, n))) +
geom_col()
mts %>%
unnest_ngrams(trigram, transcription, n=3) %>%
count(trigram, sort = TRUE) %>%
top_n(20, n) %>%
ggplot(aes(n, fct_reorder(trigram, n))) +
geom_col()
mtsample %>%
unnest_ngrams(bigram, transcription, n=2) %>%
count(bigram, sort = TRUE) %>%
top_n(20, n) %>%
ggplot(aes(n, fct_reorder(bigram, n))) +
geom_col()
mts %>%
unnest_ngrams(trigram, transcription, n=3) %>%
count(trigram, sort = TRUE) %>%
top_n(20, n) %>%
ggplot(aes(n, fct_reorder(trigram, n))) +
geom_col()
ptbigram <- mtsample %>%
unnest_ngrams(bigram, transcription, n = 2) %>%
separate(bigram, into = c("word1", "word2"), sep = " ") %>%
select(word1, word2) %>%
filter(word1 == "patient" | word2 == "patient")
library(dplyr)
library(ggplot2)
library(tidytext)
library(forcats)
library(tidyverse)
ptbigram <- mtsample %>%
unnest_ngrams(bigram, transcription, n = 2) %>%
separate(bigram, into = c("word1", "word2"), sep = " ") %>%
select(word1, word2) %>%
filter(word1 == "patient" | word2 == "patient")
ptbigram %>%
filter(word2 == "patient") %>%
count(word1, sort = TRUE) %>%
anti_join(stop_words, by = c("word1" = "word")) %>%
top_n(10) %>%
knitr::kable()
ptbigram %>%
filter(word1 == "patient") %>%
count(word2, sort = TRUE) %>%
anti_join(stop_words, by = c("word2" = "word")) %>%
top_n(10) %>%
knitr::kable()
mtsample %>%
unnest_tokens(word, transcription) %>%
group_by(medical_specialty) %>%
count(word, sort = TRUE) %>%
filter(!(word %in% stop_words$word) & !grepl(pattern = "^[0-9]+$", x = word)) %>%
top_n(5, n) %>%
arrange(medical_specialty, desc(n)) %>%
knitr::kable()
setwd("~/Desktop/PM566")
